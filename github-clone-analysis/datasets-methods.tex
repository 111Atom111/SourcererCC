In this section we explain datasets, techniques and algorithms together with the set and order of directives used to present the results of this work. The source code for the clone analysis software used can be found at \url{https://github.com/Mondego/SourcererCC}; the version used was the one of commit \com{COMMIT NUMBER OR DATE}. The statistical analysis was performed with RStudio Desktop Open Source Edition\footnote{\url{https://www.rstudio.com/products/rstudio}} and can be found on \com{SOMEWHERE}. 

\subsection{Datasets}
All the source code in Java and C/C++ was extracted in the context of the project Mining and Understanding Software Enclaves (MUSE)\footnote{\url{http://www.darpa.mil/program/mining-and-understanding-software-enclaves}}, by the Defense Advanced Research Projects Agency (DARPA)\footnote{\url{http://www.darpa.mil}}. Software projects were obtained on a straightforward and simple process; they were gathered using direct searches through the GitHub API\footnote{\url{https://developer.github.com}}, and results were afterwards crawled and downloaded. \com{The JavaScript code was obtained by...} \todo{This might change if we analyze all the projects}

Github forked repositories were filtered (to avoid biasing duplication projects) and all the projects associated with a specific language are defined in GitHub as having in majority source code in that language\footnote{\url{https://github.com/blog/1037-highlighting-repository-languages}} (in alternative to choose all projects that at least some source code, even residual). 

\begin{table}[]
\centering
\caption{Dataset statistics.}
\label{table:dataset-states}
\begin{tabular}{| l | r | r | r |} \hline
\cellcolor{black} & Java & C/C++ & JavaScript \\ \hline \hline
\# projects & 54,627 & 62,226 & \\ \hline
\# files & 5,856,349 & 35,478,611 & \\ \hline
\end{tabular}
\end{table}

In Table \ref{table:dataset-states} we can see basic information regarding the size of the datasets. \todo{Show file distributions per project, lines per file etc be here?}

%It is noticeable there are size differences between the number of projects and the number of files for Java and C/C++. On a statistical analysis of this nature it is desirable that the number of the samples to be similar between whatever subjects are being analyzed. In this particular case we face a difficulty: leveling the number of files will create a large discrepancy on the number of projects, and vice versa. Therefore, the size differences reflect a trade-of, with emphasis on minimizing the size different on projects given the absolute values for files are considerably larger.

\subsection{Tokenization}

All the analyzed source code went through a pre-processing phase where every source code file was fragmented into it's lexical constituents. Tokenization is therefore the process of transforming a file into a 'bag of words', where each individual word (token, from now on) on the file is isolated and grouped by frequency. Consider the following Java program below:

\begin{lstlisting}[language=Java,label={lst:java-example},caption={Java Foo.}]
package foo;
public class Foo {

  // Example Class for ICSE paper

  private int x;
  
  public Foo(int x)
  { this.x = x; }
  
  private void print() {
    System.out.println("Number: " + x)
  }
  
  public static void main() {
    new FooNumber(4).print();
  }
}
\end{lstlisting}

The tokenization of this program will first remove comments and terminal symbols from the language, and then group the resulting tokens by their frequency, generating the following result:

\begin{lstlisting}[numbers=none]
Java Foo:[(package,1),(foo,1),(public,3),
(class,1),(Foo,2),(private,2),(int,2),(x,5),
(this,1),(void,2),(print,2),(System,1),
(out,1),(println,1),(Number,1),(static,1),
(main,1),(new,1),(FooNumber,1),(4,1)]
\end{lstlisting}

\noindent
where the tokens '\texttt{package}' and '\texttt{foo}' appear once, the token '\texttt{public}' appears three times, and so on. Note that order is not important on the tokenized form. Unless explicitly made clear otherwise, whenever we show a comparison between two files or projects, we are \textbf{always} referring to their tokenized forms. Therefore, a software project is represented in our setting as the collection of all the tokenized forms of it's source code files. Also note that the tokenizations dismisses code comments.

\subsection{Token-Hash Reduction}

The list of the tokenized files is all the input the clone detection tool (SourcererCC) needs, but since this is the most time-consuming step in the pipeline \todo{Since I am saying this, maybe it would be interesting to keep those numbers. Tokenization times and CC times.} we had a prior optimization step in order to reduce the amount of the input. Specifically, this step grouped all the files whose tokenized form is the exact same, and send only a representative to clone analysis.

Since all the files inside a group of hash equal files are file clones between themselves, all the combinations of two files inside a group of hash-equal files represent a file-level clone pair. From this, we would only need to understand that every file clone pair return by clone analysis would not really represent a file clone pair, but rather a hash-group clone relation, with the left and right sides of the pairs representing keys to their respective groups.

Formalizing, for any three files A, B and C:

$$ if\ hash(A) = hash(C),$$ $$\\ clones(A,B)\ \Rightarrow clones(C,B) $$

In order to apply this reduction we calculated the hash value of all the files in a certain language (i.e., the hash value of all the \textbf{tokenized} files), group them by equal hash, and create a list of files representing unique, distinct hashes.

To calculate the hash of the files we used Python's hashlib (version 2.5) which ``implements a common interface to many different secure hash and message digest algorithms''\footnote{\url{https://docs.python.org/2/library/hashlib.html}}. In particular we used the MD5 hash function, which creates a 128-bit hash that we later converted into a hexadecimal form (to avoid uncommon, non-ascii characters interfering with the intermediate textual data we generated).

\com{We have at some point to explain the code used for JavaScript is not exactly the same as used for Java/C++. Perhaps here explain that we both tested with sample inputs from raw to the database and the R scripts are the same can deter a more immediate criticism (both input and output are trivial to interpret and the process is straightforward for each file in both cases).}

% I found this to be necessary
%A final note, MD5 algorithms are known to be susceptible to hash collisions, but although relevant in cryptography, these are usually extremely unlikely. Given the orders of magnitude required for the likelihood of a collision, we simply considered this negligible for this statistical work.

\subsection{SourcererCC}

SourcererCC\cite{DBLP:journals/corr/SajnaniSSRL15} is a clone detection tool developed with scalability as a main priority, and uses an heuristic-optimized index of tokens to significantly reduce the number of code-block comparisons needed to detect the clones, as well as the number of required token-comparisons needed to judge a potential clone. Testing has proved the tool to scale to a large repository with 250M lines of code in a single workstation (3Ghz i7 CPU, 12Gb RAM), and the values of precision to be 86\% and recall 93\%\footnote{Specific hardware details and testing conditions are in the original article. The value of recall varies under certain types of cloning.}.

\com{Explain in two or three paragraphs how SourcererCC works (token order by frequency, quick analysis of 20\% of the tokens for a threshold if 80\%, etc)}

All the files that were successfully tokenized are represented in the input to SourcererCC, with the exception of files whose number of tokens was less than 1, as this can be considered for our application lexically empty, even if they have code comments inside.

The tool was configured to detect file clones at 80\% or more of similarity. No clone similarity is provided by the tool regarding two file clones; rather, the tool outputs a list of all file clone pairs, being all of these at ‘80\% similarity or more’. Remember, this tool receives only the files that represent unique hashes.

\todo{I don't think it's necessary here to explain how we created shards for SourcererCC. I give emphasis on hash isolation and union because the notion of 100\% clones by hash will be an important part of presenting, analyzing and discussing the results.}

\subsection{File-level to Project-level information}

With SourcererCC successfully ran, the only step is to abstract one level to obtain project-level information. In particular, on this step we want to retrieve information regarding projects that exist inside other projects (clones) and projects that contains within themselvs other projects (hosts). A simple way of visualizing this is a small Java dependency (clone) being copied integrally to a larger project (host).

In particular, this step produces results of the form:

$$ A\ cloned\ in\ B\ at\ 75\%,\ B\ being\ affected\ by\ 20\%\ $$

where the project A does not exist integrally in B, but rather only 90\% of it's files were found in B, and over all the files in B 14\% have at least one clone in A.

Calculating project-level information is done in four steps. On the first one we simply collect all the files from a project \texttt{A} (which in this case contains 4 files):

\begin{lstlisting}[mathescape, numbers=none]
project A
  File$_1$ - 
  File$_2$ - 
  File$_3$ - 
  File$_4$ - 
\end{lstlisting}
%
after which we find the file clones returned by SourcererCC (in this case only $File_1$ and $File_2$ contain clones:
\begin{lstlisting}[mathescape, numbers=none]
project A
  File$_1$ - Clone$_{1}$
  File$_2$ - Clone$_{1}$
  File$_3$ - 
  File$_4$ - 
\end{lstlisting}
%
After having the files and their clones, we need to find which token-hash groups these files represent and extend the mapping of file clones. This operation needs to be performed for the files from project \texttt{A} and for the clones:

\begin{lstlisting}[mathescape, numbers=none]
project A
  File$_1$ - Clone$_{1}$, TokenClone$_1$, TokenClone$_2$
  File$_2$ - Clone$_{1}$,
  File$_3$ - 
  File$_4$ - TokenClone$_3$, TokenClone$_4$, TokenClone$_5$
\end{lstlisting}
%
Note in this last step that \texttt{Clone}$_1$ had 2 \texttt{TokenClones}, therefore by association so does \texttt{File}$_1$. Neither \texttt{File}$_2$ or \texttt{File}$_3$ had \texttt{TokenClones}, and \texttt{File}$_4$ has 3.

The fourth and last step is simply to change this mapping of files into a mapping to their respective projects:

\begin{lstlisting}[mathescape, numbers=none]
project A
  File$_1$ - C, B, B
  File$_2$ - B,
  File$_3$ - 
  File$_4$ - B, D, F
\end{lstlisting}
%
So in this case there are 3 files from \texttt{A} with clones in \texttt{B}, making \texttt{A} a clone at 75\% and, assuming \texttt{B} has a total of 20 files, we say this host is affected by 20\% by it's host \texttt{A} (4 files out of 20 have a cloning relation with files from \texttt{A}, are 'affected' by it).

\subsection{MySQL Database}

\com{Explain the schema etc...}
