\com{Needs redone, I just did a quick mental dump.}

For statistical works size is a relevant dimensions, thus software analysis relies more and more on large datasets of software.
Relying on large datasets of software such as BOA, Sourcerer or the entire infrastructure created
for the MUSE program is good because it reduces risks of under-representativeness of some niches.

As we have seen in out [accepted] SCAM paper, there are risks inherent to large datasets, but statistically
size is relevant and despite the recognized concerns, an observation derived from thousands of units is generally
better than one derived from a few dozens.

Software analysis poses a special risk for analysis, that grows with the size of the dataset. A statistical work on
human population or traffic has the advantage of a guaranteed assurance the units are completely independent. The biological
processes of persons or the mechanical and electronic constituents of automobiles do not intersect in any single way.
The respiratory system of a person is completely free of influences from another respiratory system, same is
true for a car's brake systems, etc.  For software this is not the case.

Dependencies is an obvious case. Software is relying more and more on general software and frameworks. Apache commons is
used in thousand of software projects, Oracle's SDK is universally used by any Java project, Google's JQuery by most websites.
Most modern operative systems have advanced systems just to manage dependencies, such as yum or apt-get on LINUX or homebrew
for Apple OS's. Most languages have central repositories and tools to manage dependencies and allow easy integration of artifacts such as
hackage for Haskell, Maven for Java, pypi for Python, rubygems for ruby, etc.

Another important phenomenon of intra-programs similarities is code cloning. Either through copy-paste of small pieces of code or through the
purposed addition of source code files form different origins to one's project, different programs can possibly share the same source code.

This is a studied phenomenon [many references from works from Mondego group], and programs are known to have pieces of code from
online sources such as stack overflow. We know cases of methods duplication in Java, function duplication in Python, etc.

This phenomenon has never been studied in large datasets of software, but has the potentially to impact current software analysis methodologies and
trends of using large datasets because code duplication will influence statistical analysis, and pieces of code or source files that are highly cloned
will bias results towards their own characteristics.

Thus, we expect to answer the following question: How does the code cloning phenomenon affect large datasets of software, through which processes
does it appear, and more important can it impact large-scale software analysis works?

In order to answer this question we analyzed the existence of code cloning on projects obtained through the well-known website GitHub, through the
analysis of projects from 4 different programming languages.

\todo{Important to say that scripts, SourcererCC and lists of file-level and project-level cloning are publicly available.}